{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb164bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081df180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import html5lib\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002b9756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Python Developer\n"
     ]
    }
   ],
   "source": [
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = bs(response.content, 'html.parser')\n",
    "first_job_title = soup.find(\"h2\", class_=\"title\").get_text()\n",
    "print(first_job_title )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73306887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senior Python Developer', 'Energy engineer', 'Legal executive', 'Fitness centre manager', 'Product manager', 'Medical technical officer', 'Physiological scientist', 'Textile designer', 'Television floor manager', 'Waste management officer', 'Software Engineer (Python)', 'Interpreter', 'Architect', 'Meteorologist', 'Audiological scientist', 'English as a second language teacher', 'Surgeon', 'Equities trader', 'Newspaper journalist', 'Materials engineer', 'Python Programmer (Entry-Level)', 'Product/process development scientist', 'Scientist, research (maths)', 'Ecologist', 'Materials engineer', 'Historic buildings inspector/conservation officer', 'Data scientist', 'Psychiatrist', 'Structural engineer', 'Immigration officer', 'Python Programmer (Entry-Level)', 'Neurosurgeon', 'Broadcast engineer', 'Make', 'Nurse, adult', 'Air broker', 'Editor, film/video', 'Production assistant, radio', 'Engineer, communications', 'Sales executive', 'Software Developer (Python)', 'Futures trader', 'Tour manager', 'Cytogeneticist', 'Designer, multimedia', 'Trade union research officer', 'Chemist, analytical', 'Programmer, multimedia', 'Engineer, broadcasting (operations)', 'Teacher, primary school', 'Python Developer', 'Manufacturing systems engineer', 'Producer, television/film/video', 'Scientist, forensic', 'Bonds trader', 'Editorial assistant', 'Photographer', 'Retail banker', 'Jewellery designer', 'Ophthalmologist', 'Back-End Web Developer (Python, Django)', 'Licensed conveyancer', 'Futures trader', 'Counselling psychologist', 'Insurance underwriter', 'Engineer, automotive', 'Producer, radio', 'Dispensing optician', 'Designer, fashion/clothing', 'Chartered loss adjuster', 'Back-End Web Developer (Python, Django)', 'Forest/woodland manager', 'Clinical cytogeneticist', 'Print production planner', 'Systems developer', 'Graphic designer', 'Writer', 'Field seismologist', 'Chief Strategy Officer', 'Air cabin crew', 'Python Programmer (Entry-Level)', 'Warden/ranger', 'Sports therapist', 'Arts development officer', 'Printmaker', 'Health and safety adviser', 'Manufacturing systems engineer', 'Programmer, applications', 'Medical physicist', 'Media planner', 'Software Developer (Python)', 'Surveyor, land/geomatics', 'Legal executive', 'Librarian, academic', 'Barrister', 'Museum/gallery exhibitions officer', 'Radiographer, diagnostic', 'Database administrator', 'Furniture designer', 'Ship broker']\n"
     ]
    }
   ],
   "source": [
    "soup = bs(response.content, 'html.parser')\n",
    "\n",
    "job_titles_elements = soup.find_all(\"h2\", class_=\"title\")\n",
    "    \n",
    "\n",
    "job_titles = [title.get_text() for title in job_titles_elements]\n",
    "print(job_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2b5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Python Developer',\n",
       " 'Energy engineer',\n",
       " 'Legal executive',\n",
       " 'Fitness centre manager',\n",
       " 'Product manager',\n",
       " 'Medical technical officer',\n",
       " 'Physiological scientist',\n",
       " 'Textile designer',\n",
       " 'Television floor manager',\n",
       " 'Waste management officer',\n",
       " 'Software Engineer (Python)',\n",
       " 'Interpreter',\n",
       " 'Architect',\n",
       " 'Meteorologist',\n",
       " 'Audiological scientist',\n",
       " 'English as a second language teacher',\n",
       " 'Surgeon',\n",
       " 'Equities trader',\n",
       " 'Newspaper journalist',\n",
       " 'Materials engineer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Product/process development scientist',\n",
       " 'Scientist, research (maths)',\n",
       " 'Ecologist',\n",
       " 'Materials engineer',\n",
       " 'Historic buildings inspector/conservation officer',\n",
       " 'Data scientist',\n",
       " 'Psychiatrist',\n",
       " 'Structural engineer',\n",
       " 'Immigration officer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Neurosurgeon',\n",
       " 'Broadcast engineer',\n",
       " 'Make',\n",
       " 'Nurse, adult',\n",
       " 'Air broker',\n",
       " 'Editor, film/video',\n",
       " 'Production assistant, radio',\n",
       " 'Engineer, communications',\n",
       " 'Sales executive',\n",
       " 'Software Developer (Python)',\n",
       " 'Futures trader',\n",
       " 'Tour manager',\n",
       " 'Cytogeneticist',\n",
       " 'Designer, multimedia',\n",
       " 'Trade union research officer',\n",
       " 'Chemist, analytical',\n",
       " 'Programmer, multimedia',\n",
       " 'Engineer, broadcasting (operations)',\n",
       " 'Teacher, primary school',\n",
       " 'Python Developer',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Producer, television/film/video',\n",
       " 'Scientist, forensic',\n",
       " 'Bonds trader',\n",
       " 'Editorial assistant',\n",
       " 'Photographer',\n",
       " 'Retail banker',\n",
       " 'Jewellery designer',\n",
       " 'Ophthalmologist',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Licensed conveyancer',\n",
       " 'Futures trader',\n",
       " 'Counselling psychologist',\n",
       " 'Insurance underwriter',\n",
       " 'Engineer, automotive',\n",
       " 'Producer, radio',\n",
       " 'Dispensing optician',\n",
       " 'Designer, fashion/clothing',\n",
       " 'Chartered loss adjuster',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Forest/woodland manager',\n",
       " 'Clinical cytogeneticist',\n",
       " 'Print production planner',\n",
       " 'Systems developer',\n",
       " 'Graphic designer',\n",
       " 'Writer',\n",
       " 'Field seismologist',\n",
       " 'Chief Strategy Officer',\n",
       " 'Air cabin crew',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Warden/ranger',\n",
       " 'Sports therapist',\n",
       " 'Arts development officer',\n",
       " 'Printmaker',\n",
       " 'Health and safety adviser',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Programmer, applications',\n",
       " 'Medical physicist',\n",
       " 'Media planner',\n",
       " 'Software Developer (Python)',\n",
       " 'Surveyor, land/geomatics',\n",
       " 'Legal executive',\n",
       " 'Librarian, academic',\n",
       " 'Barrister',\n",
       " 'Museum/gallery exhibitions officer',\n",
       " 'Radiographer, diagnostic',\n",
       " 'Database administrator',\n",
       " 'Furniture designer',\n",
       " 'Ship broker']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "680d7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senior Python Developer', 'Energy engineer', 'Legal executive', 'Fitness centre manager', 'Product manager', 'Medical technical officer', 'Physiological scientist', 'Textile designer', 'Television floor manager', 'Waste management officer', 'Software Engineer (Python)', 'Interpreter', 'Architect', 'Meteorologist', 'Audiological scientist', 'English as a second language teacher', 'Surgeon', 'Equities trader', 'Newspaper journalist', 'Materials engineer', 'Python Programmer (Entry-Level)', 'Product/process development scientist', 'Scientist, research (maths)', 'Ecologist', 'Materials engineer', 'Historic buildings inspector/conservation officer', 'Data scientist', 'Psychiatrist', 'Structural engineer', 'Immigration officer', 'Python Programmer (Entry-Level)', 'Neurosurgeon', 'Broadcast engineer', 'Make', 'Nurse, adult', 'Air broker', 'Editor, film/video', 'Production assistant, radio', 'Engineer, communications', 'Sales executive', 'Software Developer (Python)', 'Futures trader', 'Tour manager', 'Cytogeneticist', 'Designer, multimedia', 'Trade union research officer', 'Chemist, analytical', 'Programmer, multimedia', 'Engineer, broadcasting (operations)', 'Teacher, primary school', 'Python Developer', 'Manufacturing systems engineer', 'Producer, television/film/video', 'Scientist, forensic', 'Bonds trader', 'Editorial assistant', 'Photographer', 'Retail banker', 'Jewellery designer', 'Ophthalmologist', 'Back-End Web Developer (Python, Django)', 'Licensed conveyancer', 'Futures trader', 'Counselling psychologist', 'Insurance underwriter', 'Engineer, automotive', 'Producer, radio', 'Dispensing optician', 'Designer, fashion/clothing', 'Chartered loss adjuster', 'Back-End Web Developer (Python, Django)', 'Forest/woodland manager', 'Clinical cytogeneticist', 'Print production planner', 'Systems developer', 'Graphic designer', 'Writer', 'Field seismologist', 'Chief Strategy Officer', 'Air cabin crew', 'Python Programmer (Entry-Level)', 'Warden/ranger', 'Sports therapist', 'Arts development officer', 'Printmaker', 'Health and safety adviser', 'Manufacturing systems engineer', 'Programmer, applications', 'Medical physicist', 'Media planner', 'Software Developer (Python)', 'Surveyor, land/geomatics', 'Legal executive', 'Librarian, academic', 'Barrister', 'Museum/gallery exhibitions officer', 'Radiographer, diagnostic', 'Database administrator', 'Furniture designer', 'Ship broker'] [] ['Stewartbury, AA', 'Christopherville, AA', 'Port Ericaburgh, AA', 'East Seanview, AP', 'North Jamieview, AP', 'Davidville, AP', 'South Christopher, AE', 'Port Jonathan, AE', 'Osbornetown, AE', 'Scotttown, AP', 'Ericberg, AE', 'Ramireztown, AE', 'Figueroaview, AA', 'Kelseystad, AA', 'Williamsburgh, AE', 'Mitchellburgh, AE', 'West Jessicabury, AA', 'Maloneshire, AE', 'Johnsonton, AA', 'South Davidtown, AP', 'Port Sara, AE', 'Marktown, AA', 'Laurenland, AE', 'Lauraton, AP', 'South Tammyberg, AP', 'North Brandonville, AP', 'Port Robertfurt, AA', 'Burnettbury, AE', 'Herbertside, AA', 'Christopherport, AP', 'West Victor, AE', 'Port Aaron, AP', 'Loribury, AA', 'Angelastad, AP', 'Larrytown, AE', 'West Colin, AP', 'West Stephanie, AP', 'Laurentown, AP', 'Wrightberg, AP', 'Alberttown, AE', 'Brockburgh, AE', 'North Jason, AE', 'Arnoldhaven, AE', 'Lake Destiny, AP', 'South Timothyburgh, AP', 'New Jimmyton, AE', 'New Lucasbury, AP', 'Port Cory, AE', 'Gileston, AA', 'Cindyshire, AA', 'East Michaelfort, AA', 'Joybury, AE', 'Emmatown, AE', 'Colehaven, AP', 'Port Coryton, AE', 'Amyborough, AA', 'Reynoldsville, AA', 'Port Billy, AP', 'Adamburgh, AA', 'Wilsonmouth, AA', 'South Kimberly, AA', 'Benjaminland, AP', 'Zacharyport, AA', 'Port Devonville, AE', 'East Thomas, AE', 'New Jeffrey, AP', 'Davidside, AA', 'Jamesville, AA', 'New Kelly, AP', 'Lake Antonio, AA', 'New Elizabethside, AA', 'Millsbury, AE', 'Lloydton, AP', 'Port Jeremy, AA', 'New Elizabethtown, AA', 'Charlesstad, AE', 'Josephbury, AE', 'Seanfurt, AA', 'Williambury, AA', 'South Jorgeside, AP', 'Robertborough, AP', 'South Saratown, AP', 'Hullview, AA', 'Philipland, AP', 'North Patty, AE', 'North Stephen, AE', 'Stevensland, AP', 'Reyesstad, AE', 'Bellberg, AP', 'North Johnland, AE', 'Martinezburgh, AE', 'Joshuatown, AE', 'West Ericstad, AA', 'Tuckertown, AE', 'Perezton, AE', 'Lake Abigail, AE', 'Jacobshire, AP', 'Port Susan, AE', 'North Tiffany, AA', 'Michelleville, AP'] ['2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08', '2021-04-08']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.get(url)\n",
    "soup = bs(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "# job titles\n",
    "job_titles_elements = soup.find_all(\"h2\", class_=\"title\")\n",
    "job_titles = [title.get_text().strip() for title in job_titles_elements]\n",
    "\n",
    "# companies \n",
    "companies_elements = soup.find_all(\"div\", class_=\"company\")\n",
    "companies = [company.get_text().strip() for company in companies_elements]\n",
    "\n",
    "# locations\n",
    "locations_elements = soup.find_all(\"p\", class_=\"location\")\n",
    "locations = [location.get_text().strip() for location in locations_elements]\n",
    "\n",
    "# date posted\n",
    "dates_elements = soup.find_all(\"time\", datetime=True)\n",
    "dates = [date.get_text().strip() for date in dates_elements]\n",
    "\n",
    "print(job_titles, companies, locations, dates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc3b8330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date Posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title                     Company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                Location Date Posted  \n",
       "0        Stewartbury, AA  2021-04-08  \n",
       "1   Christopherville, AA  2021-04-08  \n",
       "2    Port Ericaburgh, AA  2021-04-08  \n",
       "3      East Seanview, AP  2021-04-08  \n",
       "4    North Jamieview, AP  2021-04-08  \n",
       "..                   ...         ...  \n",
       "95      Lake Abigail, AE  2021-04-08  \n",
       "96        Jacobshire, AP  2021-04-08  \n",
       "97        Port Susan, AE  2021-04-08  \n",
       "98     North Tiffany, AA  2021-04-08  \n",
       "99     Michelleville, AP  2021-04-08  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def try_diff_way(url):\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    # Initialize job boxes\n",
    "    job_titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    posting_dates = []\n",
    "\n",
    "    \n",
    "    job_containers = soup.find_all('div', class_='card-content')\n",
    "\n",
    "    for job in job_containers:\n",
    "       \n",
    "        job_titles = [title.get_text().strip() for title in soup.find_all(\"h2\", class_=\"title\")]\n",
    "        companies = [company.get_text().strip() for company in soup.find_all(\"h3\", class_=\"company\")]\n",
    "        locations = [location.get_text().strip() for location in soup.find_all(\"p\", class_=\"location\")]\n",
    "        posting_dates = [date.get_text().strip() for date in soup.find_all(\"time\")]\n",
    "\n",
    "    # Create df\n",
    "    job_data = pd.DataFrame({\n",
    "        'Title': job_titles,\n",
    "        'Company': companies,\n",
    "        'Location': locations,\n",
    "        'Date Posted': posting_dates\n",
    "    })\n",
    "\n",
    "    return job_data\n",
    "\n",
    "\n",
    "try_diff_way = try_diff_way(url)\n",
    "\n",
    "try_diff_way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0607f5",
   "metadata": {},
   "source": [
    "2.2. Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls.  \n",
    "    b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f8f5525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Title                     Company              Location  \\\n",
      "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA   \n",
      "1          Energy engineer            Vasquez-Davidson  Christopherville, AA   \n",
      "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA   \n",
      "3   Fitness centre manager              Savage-Bradley     East Seanview, AP   \n",
      "4          Product manager                 Ramirez Inc   North Jamieview, AP   \n",
      "\n",
      "  Date Posted                                         Apply Link  \n",
      "0  2021-04-08  https://realpython.github.io/fake-jobs/jobs/se...  \n",
      "1  2021-04-08  https://realpython.github.io/fake-jobs/jobs/en...  \n",
      "2  2021-04-08  https://realpython.github.io/fake-jobs/jobs/le...  \n",
      "3  2021-04-08  https://realpython.github.io/fake-jobs/jobs/fi...  \n",
      "4  2021-04-08  https://realpython.github.io/fake-jobs/jobs/pr...  \n"
     ]
    }
   ],
   "source": [
    "#2a\n",
    "\n",
    "def job_scrape_with_apply(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    job_titles = [title.get_text().strip() for title in soup.find_all(\"h2\", class_=\"title\")]\n",
    "    companies = [company.get_text().strip() for company in soup.find_all(\"h3\", class_=\"company\")]\n",
    "    locations = [location.get_text().strip() for location in soup.find_all(\"p\", class_=\"location\")]\n",
    "    posting_dates = [date.get_text().strip() for date in soup.find_all(\"time\")]\n",
    "\n",
    "    # THE DARN LINKS\n",
    "    apply_links = [link['href'] for link in soup.find_all('a', class_='card-footer-item', string='Apply')]\n",
    "\n",
    "    \n",
    "\n",
    "    # make df \n",
    "    job_data = pd.DataFrame({\n",
    "        'Title': job_titles,\n",
    "        'Company': companies,\n",
    "        'Location': locations,\n",
    "        'Date Posted': posting_dates,\n",
    "        'Apply Link': apply_links\n",
    "        \n",
    "    })\n",
    "\n",
    "    return job_data\n",
    "\n",
    "\n",
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "job_listings_df = job_scrape_with_apply(url)\n",
    "print(job_listings_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b1a5d",
   "metadata": {},
   "source": [
    "3. Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph.  \n",
    "    b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\".  \n",
    "    c. Use the [.apply method](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa1167b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.\n"
     ]
    }
   ],
   "source": [
    "def get_first_job_description(url1):\n",
    "    response = requests.get(url1)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    job_description_paragraphs = soup.find_all('p')  \n",
    "\n",
    "   ## [1] is going to point to the 2nd <p> paragraph tag\n",
    "    job_description = job_description_paragraphs[1].get_text().strip()  \n",
    " \n",
    "    return job_description\n",
    "\n",
    "url1 = \"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\"\n",
    "desc1 = get_first_job_description(url1)\n",
    "print(desc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b297e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
